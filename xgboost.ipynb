{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "duplicate-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn import linear_model, svm, neighbors, preprocessing, metrics, model_selection, ensemble, multiclass, neural_network\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import imblearn\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qualified-coalition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3156: DtypeWarning: Columns (19) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831</td>\n",
       "      <td>0.814</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.364</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.3890</td>\n",
       "      <td>156.985</td>\n",
       "      <td>124539</td>\n",
       "      <td>4</td>\n",
       "      <td>Dark Trap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.719</td>\n",
       "      <td>0.493</td>\n",
       "      <td>8</td>\n",
       "      <td>-7.230</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>115.080</td>\n",
       "      <td>224427</td>\n",
       "      <td>4</td>\n",
       "      <td>Dark Trap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.893</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.783</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.3720</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>218.050</td>\n",
       "      <td>98821</td>\n",
       "      <td>4</td>\n",
       "      <td>Dark Trap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.476</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.710</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>186.948</td>\n",
       "      <td>123661</td>\n",
       "      <td>3</td>\n",
       "      <td>Dark Trap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.624</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.668</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>0.2170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>147.988</td>\n",
       "      <td>123298</td>\n",
       "      <td>4</td>\n",
       "      <td>Dark Trap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.831   0.814    2    -7.364     1       0.4200        0.0598   \n",
       "1         0.719   0.493    8    -7.230     1       0.0794        0.4010   \n",
       "2         0.850   0.893    5    -4.783     1       0.0623        0.0138   \n",
       "3         0.476   0.781    0    -4.710     1       0.1030        0.0237   \n",
       "4         0.798   0.624    2    -7.668     1       0.2930        0.2170   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  duration_ms  time_signature  \\\n",
       "0          0.013400    0.0556   0.3890  156.985       124539               4   \n",
       "1          0.000000    0.1180   0.1240  115.080       224427               4   \n",
       "2          0.000004    0.3720   0.0391  218.050        98821               4   \n",
       "3          0.000000    0.1140   0.1750  186.948       123661               3   \n",
       "4          0.000000    0.1660   0.5910  147.988       123298               4   \n",
       "\n",
       "       genre  \n",
       "0  Dark Trap  \n",
       "1  Dark Trap  \n",
       "2  Dark Trap  \n",
       "3  Dark Trap  \n",
       "4  Dark Trap  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 789\n",
    "genres_df = pd.read_csv(\"src/data/genres_v2.csv\")\n",
    "playlist_df = pd.read_csv(\"src/data/playlists.csv\")\n",
    "\n",
    "# data cleaning\n",
    "take_notna = lambda s1,s2: s1 if type(s1) == str else s2\n",
    "genres_df[\"song_title\"]=genres_df[\"song_name\"].combine(genres_df[\"title\"],take_notna, fill_value=None)\n",
    "genres_df.drop(columns = [\"song_name\",\"title\",\"Unnamed: 0\"], inplace=True)\n",
    "genres_df.dropna(inplace=True)\n",
    "# changing dtype\n",
    "genres_df[\"key\"] = genres_df[\"key\"].astype(int)\n",
    "genres_df[\"mode\"] = genres_df[\"mode\"].astype(int)\n",
    "genres_df[\"duration_ms\"] = genres_df[\"duration_ms\"].astype(int)\n",
    "genres_df[\"time_signature\"] = genres_df[\"time_signature\"].astype(int)\n",
    "# selecting columns to use\n",
    "valid_columns = [\"danceability\",\"energy\",\"key\",\"loudness\",\"mode\",\"speechiness\",\"acousticness\",\"instrumentalness\",\n",
    "                \"liveness\",\"valence\",\"tempo\",\"duration_ms\",\"time_signature\",\"genre\"]\n",
    "genres_df = genres_df[valid_columns].reset_index(drop=True)\n",
    "\n",
    "genres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabulous-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_selection.train_test_split(genres_df, random_state = RANDOM_STATE)\n",
    "train_data.reset_index(drop = True, inplace = True)\n",
    "test_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# train_data_sub, val_data = model_selection.train_test_split(train_data, random_state = RANDOM_STATE)\n",
    "# train_data_sub.reset_index(drop = True, inplace = True)\n",
    "# val_data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "signal-judge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# encoding target variable\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_label = le.fit_transform(train_data[\"genre\"])\n",
    "test_label = le.transform(test_data[\"genre\"])\n",
    "train_data[\"labels\"] = train_label\n",
    "test_data[\"labels\"] = test_label\n",
    "\n",
    "# create val dataset\n",
    "train_data_sub, val_data = model_selection.train_test_split(train_data, test_size = 0.1, random_state = RANDOM_STATE)\n",
    "train_data_sub.reset_index(drop = True, inplace = True)\n",
    "val_data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "amber-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing datasets to train on xgboost\n",
    "data_cols = []\n",
    "for col in train_data.columns.tolist():\n",
    "    if col == \"genre\" or col ==\"labels\":\n",
    "        pass\n",
    "    else:\n",
    "        data_cols.append(col)\n",
    "dtrain = xgb.DMatrix(data = train_data[data_cols], label = train_data[\"labels\"])\n",
    "dtest = xgb.DMatrix(data = test_data[data_cols], label = test_data[\"labels\"])\n",
    "dtrain_sub = xgb.DMatrix(data = train_data_sub[data_cols], label = train_data_sub[\"labels\"])\n",
    "dval = xgb.DMatrix(data = val_data[data_cols], label = val_data[\"labels\"])\n",
    "# saving to binary to decrease the loading speed\n",
    "dtrain.save_binary(\"src/data/xbg_train.buffer\")\n",
    "dtest.save_binary(\"src/data/xgb_test.buffer\")\n",
    "dtrain_sub.save_binary(\"src/data/xgb_train_sub.buffer\")\n",
    "dval.save_binary(\"src/data/xgb_val.buffer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "union-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training xgboost\n",
    "num_classes = len(train_data[\"labels\"].unique())\n",
    "param = {\"max_depth\":6,\n",
    "         \"eta\":0.1,\n",
    "         \"nthread\":-1,\n",
    "         \"gpu_id\":0,\n",
    "         \"tree_method\":\"gpu_hist\",\n",
    "         \"objective\":\"multi:softmax\",\n",
    "         \"num_class\":num_classes,\n",
    "         \"eval_metric\":\"auc\"}\n",
    "n_rounds = 100\n",
    "bst = xgb.train(param, dtrain, n_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "patient-lindsay",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Dark Trap       0.73      0.59      0.65      3445\n",
      "            Emo       0.84      0.83      0.83      1265\n",
      "         Hiphop       0.64      0.58      0.61      2275\n",
      "            Pop       0.81      0.32      0.46       344\n",
      "            Rap       0.89      0.36      0.52      1401\n",
      "            RnB       0.61      0.61      0.61      1558\n",
      "     Trap Metal       0.69      0.46      0.55      1488\n",
      "Underground Rap       0.52      0.76      0.62      4380\n",
      "            dnb       0.99      1.00      0.99      2250\n",
      "      hardstyle       0.91      0.97      0.93      2205\n",
      "      psytrance       0.96      0.95      0.95      2172\n",
      "      techhouse       0.93      0.96      0.94      2278\n",
      "         techno       0.92      0.92      0.92      2204\n",
      "         trance       0.88      0.93      0.90      2216\n",
      "           trap       0.91      0.91      0.91      2243\n",
      "\n",
      "       accuracy                           0.78     31724\n",
      "      macro avg       0.81      0.74      0.76     31724\n",
      "   weighted avg       0.80      0.78      0.78     31724\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Dark Trap       0.56      0.46      0.51      1133\n",
      "            Emo       0.71      0.67      0.69       415\n",
      "         Hiphop       0.46      0.42      0.44       747\n",
      "            Pop       0.30      0.08      0.12       117\n",
      "            Rap       0.70      0.29      0.41       447\n",
      "            RnB       0.40      0.40      0.40       541\n",
      "     Trap Metal       0.48      0.31      0.38       468\n",
      "Underground Rap       0.45      0.66      0.54      1495\n",
      "            dnb       0.97      0.99      0.98       716\n",
      "      hardstyle       0.86      0.94      0.90       731\n",
      "      psytrance       0.92      0.93      0.92       789\n",
      "      techhouse       0.88      0.90      0.89       697\n",
      "         techno       0.87      0.86      0.87       752\n",
      "         trance       0.81      0.87      0.84       783\n",
      "           trap       0.87      0.83      0.85       744\n",
      "\n",
      "       accuracy                           0.69     10575\n",
      "      macro avg       0.68      0.64      0.65     10575\n",
      "   weighted avg       0.69      0.69      0.68     10575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report on xgboost classifier\n",
    "print(metrics.classification_report(train_label, bst.predict(dtrain), target_names = le.classes_))\n",
    "print(metrics.classification_report(test_label, bst.predict(dtest), target_names = le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sixth-vanilla",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.73      0.59      0.97      0.65      0.76      0.55      3445\n",
      "            Emo       0.84      0.83      0.99      0.83      0.91      0.82      1265\n",
      "         Hiphop       0.64      0.58      0.97      0.61      0.75      0.54      2275\n",
      "            Pop       0.81      0.32      1.00      0.46      0.56      0.30       344\n",
      "            Rap       0.89      0.36      1.00      0.52      0.60      0.34      1401\n",
      "            RnB       0.61      0.61      0.98      0.61      0.78      0.58      1558\n",
      "     Trap Metal       0.69      0.46      0.99      0.55      0.68      0.43      1488\n",
      "Underground Rap       0.52      0.76      0.89      0.62      0.82      0.67      4380\n",
      "            dnb       0.99      1.00      1.00      0.99      1.00      1.00      2250\n",
      "      hardstyle       0.91      0.97      0.99      0.93      0.98      0.96      2205\n",
      "      psytrance       0.96      0.95      1.00      0.95      0.97      0.94      2172\n",
      "      techhouse       0.93      0.96      0.99      0.94      0.97      0.95      2278\n",
      "         techno       0.92      0.92      0.99      0.92      0.96      0.91      2204\n",
      "         trance       0.88      0.93      0.99      0.90      0.96      0.91      2216\n",
      "           trap       0.91      0.91      0.99      0.91      0.95      0.90      2243\n",
      "\n",
      "    avg / total       0.80      0.78      0.98      0.78      0.86      0.75     31724\n",
      "\n",
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.56      0.46      0.96      0.51      0.66      0.42      1133\n",
      "            Emo       0.71      0.67      0.99      0.69      0.82      0.65       415\n",
      "         Hiphop       0.46      0.42      0.96      0.44      0.64      0.38       747\n",
      "            Pop       0.30      0.08      1.00      0.12      0.28      0.07       117\n",
      "            Rap       0.70      0.29      0.99      0.41      0.53      0.26       447\n",
      "            RnB       0.40      0.40      0.97      0.40      0.62      0.36       541\n",
      "     Trap Metal       0.48      0.31      0.98      0.38      0.55      0.28       468\n",
      "Underground Rap       0.45      0.66      0.87      0.54      0.76      0.57      1495\n",
      "            dnb       0.97      0.99      1.00      0.98      0.99      0.98       716\n",
      "      hardstyle       0.86      0.94      0.99      0.90      0.97      0.93       731\n",
      "      psytrance       0.92      0.93      0.99      0.92      0.96      0.92       789\n",
      "      techhouse       0.88      0.90      0.99      0.89      0.94      0.88       697\n",
      "         techno       0.87      0.86      0.99      0.87      0.92      0.84       752\n",
      "         trance       0.81      0.87      0.98      0.84      0.93      0.85       783\n",
      "           trap       0.87      0.83      0.99      0.85      0.91      0.81       744\n",
      "\n",
      "    avg / total       0.69      0.69      0.97      0.68      0.80      0.66     10575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imbalanced classification report on xgboost\n",
    "print(imblearn.metrics.classification_report_imbalanced(train_label, bst.predict(dtrain), target_names = le.classes_))\n",
    "print(imblearn.metrics.classification_report_imbalanced(test_label, bst.predict(dtest), target_names = le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ceramic-roads",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# listing out the metrics system available from sklearn\n",
    "metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "broadband-native",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\core.py:419: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning)\n",
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:00:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:00:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:00:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:01:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:01:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:01:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:01:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:01:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:02:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:02:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:02:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:03:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:03:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:03:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:03:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:04:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:04:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:04:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:04:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:06:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:06:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:06:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:07:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:07:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:07:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:07:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:08:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:08:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:08:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:09:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:09:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:09:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:09:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:12:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:12:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:12:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:13:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:13:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:13:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:14:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:14:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:14:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:14:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:15:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:15:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:15:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:15:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:15:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:16:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:16:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:16:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100,...\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=20, n_jobs=1,\n",
       "                   param_distributions={'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                                        'eta': [0.03, 0.01, 0.005, 0.001],\n",
       "                                        'gamma': [0, 0.5, 1, 1.5, 2, 2.5],\n",
       "                                        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                                        'min_child_weight': [1, 3, 5, 7, 10],\n",
       "                                        'reg_lambda': [0.4, 0.6, 0.8, 1, 1.2,\n",
       "                                                       1.4],\n",
       "                                        'subsample': [0.6, 0.8, 1.0]},\n",
       "                   random_state=789, scoring='roc_auc_ovr')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimize xgboost\n",
    "train_data_sub, val_data = model_selection.train_test_split(train_data, test_size = 0.1, random_state = RANDOM_STATE)\n",
    "num_classes  = len(train_data[\"genre\"].unique())\n",
    "eval_list = [(dtrain,\"train\"),(dval,\"validate\"),(dtest,\"test\")]\n",
    "param = {\"nthread\":-1,\n",
    "         \"gpu_id\":0,\n",
    "         \"tree_method\":\"gpu_hist\",\n",
    "         \"objective\":\"multi:softmax\",\n",
    "         \"num_class\":num_classes,\n",
    "         \"eval_metric\":\"multi:softmax\",\n",
    "         \"use_label_encoder\":False,\n",
    "        \"seed\":RANDOM_STATE}\n",
    "xgb_pipe = xgb.XGBClassifier(param)\n",
    "\n",
    "params = {\"eta\":[0.03, 0.01, 0.005, 0.001],\n",
    "         \"min_child_weight\":[1,3,5,7,10],\n",
    "         \"gamma\":[0, 0.5, 1, 1.5, 2, 2.5],\n",
    "         \"subsample\":[0.6,0.8, 1.0],\n",
    "         \"colsample_bytree\":[0.6,0.8, 1.0],\n",
    "         \"max_depth\":[3,4,5,6,7,8,9,10],\n",
    "         \"reg_lambda\":[0.4, 0.6, 0.8, 1, 1.2, 1.4]}\n",
    "fit_params = {\"early_stopping_rounds\":10,\n",
    "             \"eval_set\":[(val_data[data_cols],val_data[\"labels\"])],\n",
    "             \"eval_metric\":\"multi:softmax\"}\n",
    "\n",
    "rs_clf = model_selection.RandomizedSearchCV(xgb_pipe, params, scoring=\"roc_auc_ovr\",\n",
    "                                  n_jobs=1,n_iter=20, cv=3,\n",
    "                                   random_state = RANDOM_STATE)\n",
    "\n",
    "rs_clf.fit(train_data[data_cols],train_data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ready-interval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.68      0.54      0.97      0.61      0.73      0.50      3114\n",
      "            Emo       0.82      0.77      0.99      0.79      0.87      0.75      1140\n",
      "         Hiphop       0.63      0.56      0.97      0.59      0.74      0.52      2064\n",
      "            Pop       0.81      0.13      1.00      0.22      0.36      0.12       300\n",
      "            Rap       0.95      0.30      1.00      0.45      0.55      0.28      1276\n",
      "            RnB       0.57      0.54      0.98      0.55      0.73      0.51      1398\n",
      "     Trap Metal       0.68      0.39      0.99      0.49      0.62      0.36      1336\n",
      "Underground Rap       0.49      0.77      0.87      0.60      0.82      0.66      3948\n",
      "            dnb       0.97      0.99      1.00      0.98      1.00      0.99      2026\n",
      "      hardstyle       0.88      0.95      0.99      0.91      0.97      0.94      1977\n",
      "      psytrance       0.95      0.94      1.00      0.94      0.97      0.93      1932\n",
      "      techhouse       0.91      0.94      0.99      0.93      0.97      0.93      2055\n",
      "         techno       0.90      0.90      0.99      0.90      0.95      0.89      1999\n",
      "         trance       0.84      0.91      0.99      0.87      0.95      0.89      1969\n",
      "           trap       0.88      0.89      0.99      0.88      0.94      0.87      2017\n",
      "\n",
      "    avg / total       0.77      0.75      0.97      0.74      0.84      0.72     28551\n",
      "\n",
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.57      0.45      0.96      0.50      0.66      0.41      1133\n",
      "            Emo       0.70      0.63      0.99      0.66      0.79      0.60       415\n",
      "         Hiphop       0.48      0.43      0.96      0.45      0.65      0.39       747\n",
      "            Pop       0.31      0.03      1.00      0.06      0.18      0.03       117\n",
      "            Rap       0.93      0.27      1.00      0.42      0.52      0.25       447\n",
      "            RnB       0.41      0.37      0.97      0.39      0.60      0.34       541\n",
      "     Trap Metal       0.51      0.29      0.99      0.37      0.53      0.26       468\n",
      "Underground Rap       0.45      0.70      0.86      0.55      0.78      0.59      1495\n",
      "            dnb       0.95      0.98      1.00      0.97      0.99      0.98       716\n",
      "      hardstyle       0.83      0.93      0.99      0.88      0.96      0.91       731\n",
      "      psytrance       0.91      0.92      0.99      0.91      0.95      0.90       789\n",
      "      techhouse       0.87      0.89      0.99      0.88      0.94      0.88       697\n",
      "         techno       0.86      0.86      0.99      0.86      0.92      0.84       752\n",
      "         trance       0.80      0.87      0.98      0.83      0.92      0.84       783\n",
      "           trap       0.85      0.82      0.99      0.83      0.90      0.80       744\n",
      "\n",
      "    avg / total       0.70      0.69      0.97      0.68      0.80      0.65     10575\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_eta</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19.005445</td>\n",
       "      <td>0.294881</td>\n",
       "      <td>0.137581</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 1.2, 'min_chi...</td>\n",
       "      <td>0.950181</td>\n",
       "      <td>0.950860</td>\n",
       "      <td>0.950343</td>\n",
       "      <td>0.950461</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23.500025</td>\n",
       "      <td>0.129350</td>\n",
       "      <td>0.146319</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 0.6, 'min_chi...</td>\n",
       "      <td>0.947933</td>\n",
       "      <td>0.948925</td>\n",
       "      <td>0.948339</td>\n",
       "      <td>0.948399</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.038350</td>\n",
       "      <td>0.098154</td>\n",
       "      <td>0.143256</td>\n",
       "      <td>0.014478</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 0.6, 'min_chi...</td>\n",
       "      <td>0.947627</td>\n",
       "      <td>0.948614</td>\n",
       "      <td>0.948234</td>\n",
       "      <td>0.948158</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.112634</td>\n",
       "      <td>0.768110</td>\n",
       "      <td>0.120299</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 1.4, 'min_chi...</td>\n",
       "      <td>0.947403</td>\n",
       "      <td>0.948768</td>\n",
       "      <td>0.948019</td>\n",
       "      <td>0.948064</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.987884</td>\n",
       "      <td>0.055443</td>\n",
       "      <td>0.117686</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'reg_lambda': 1, 'min_child...</td>\n",
       "      <td>0.948115</td>\n",
       "      <td>0.948486</td>\n",
       "      <td>0.947454</td>\n",
       "      <td>0.948018</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25.087349</td>\n",
       "      <td>0.098218</td>\n",
       "      <td>0.139582</td>\n",
       "      <td>0.013390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 0.4, 'min_chi...</td>\n",
       "      <td>0.947056</td>\n",
       "      <td>0.947549</td>\n",
       "      <td>0.946818</td>\n",
       "      <td>0.947141</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.301558</td>\n",
       "      <td>0.157031</td>\n",
       "      <td>0.104106</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 0.6, 'min_chi...</td>\n",
       "      <td>0.945127</td>\n",
       "      <td>0.946633</td>\n",
       "      <td>0.946009</td>\n",
       "      <td>0.945923</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.022394</td>\n",
       "      <td>0.057102</td>\n",
       "      <td>0.120369</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 1, 'min_child...</td>\n",
       "      <td>0.944343</td>\n",
       "      <td>0.945689</td>\n",
       "      <td>0.944817</td>\n",
       "      <td>0.944949</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19.809258</td>\n",
       "      <td>0.091378</td>\n",
       "      <td>0.120953</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 0.4, 'min_chi...</td>\n",
       "      <td>0.943793</td>\n",
       "      <td>0.944025</td>\n",
       "      <td>0.944904</td>\n",
       "      <td>0.944241</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.385061</td>\n",
       "      <td>0.076978</td>\n",
       "      <td>0.120643</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.8, 'reg_lambda': 1, 'min_child...</td>\n",
       "      <td>0.943390</td>\n",
       "      <td>0.945132</td>\n",
       "      <td>0.943979</td>\n",
       "      <td>0.944167</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.573562</td>\n",
       "      <td>0.168787</td>\n",
       "      <td>0.096001</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 0.8, 'min_chi...</td>\n",
       "      <td>0.942501</td>\n",
       "      <td>0.943380</td>\n",
       "      <td>0.942942</td>\n",
       "      <td>0.942941</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.301050</td>\n",
       "      <td>0.136761</td>\n",
       "      <td>0.115175</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 0.8, 'min_chi...</td>\n",
       "      <td>0.940976</td>\n",
       "      <td>0.940614</td>\n",
       "      <td>0.942232</td>\n",
       "      <td>0.941274</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.816092</td>\n",
       "      <td>0.286850</td>\n",
       "      <td>0.139232</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 1.2, 'min_chi...</td>\n",
       "      <td>0.935796</td>\n",
       "      <td>0.935603</td>\n",
       "      <td>0.937142</td>\n",
       "      <td>0.936180</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19.623811</td>\n",
       "      <td>0.229109</td>\n",
       "      <td>0.119622</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 0.6, 'min_chi...</td>\n",
       "      <td>0.935788</td>\n",
       "      <td>0.935769</td>\n",
       "      <td>0.936873</td>\n",
       "      <td>0.936144</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21.376662</td>\n",
       "      <td>0.299198</td>\n",
       "      <td>0.122625</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 1.4, 'min_chi...</td>\n",
       "      <td>0.936308</td>\n",
       "      <td>0.935201</td>\n",
       "      <td>0.936711</td>\n",
       "      <td>0.936073</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.064271</td>\n",
       "      <td>0.025927</td>\n",
       "      <td>0.099042</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 1.2, 'min_chi...</td>\n",
       "      <td>0.935148</td>\n",
       "      <td>0.936426</td>\n",
       "      <td>0.936339</td>\n",
       "      <td>0.935971</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.424132</td>\n",
       "      <td>0.170260</td>\n",
       "      <td>0.102681</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 1, 'min_child...</td>\n",
       "      <td>0.931463</td>\n",
       "      <td>0.931666</td>\n",
       "      <td>0.933989</td>\n",
       "      <td>0.932373</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.499972</td>\n",
       "      <td>0.030676</td>\n",
       "      <td>0.095751</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'reg_lambda': 0.4, 'min_chi...</td>\n",
       "      <td>0.930889</td>\n",
       "      <td>0.931758</td>\n",
       "      <td>0.930827</td>\n",
       "      <td>0.931158</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.088888</td>\n",
       "      <td>0.065485</td>\n",
       "      <td>0.102079</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 1.2, 'min_chi...</td>\n",
       "      <td>0.926852</td>\n",
       "      <td>0.926982</td>\n",
       "      <td>0.927126</td>\n",
       "      <td>0.926987</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.921600</td>\n",
       "      <td>0.173972</td>\n",
       "      <td>0.095163</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 1.4, 'min_chi...</td>\n",
       "      <td>0.919542</td>\n",
       "      <td>0.917448</td>\n",
       "      <td>0.921817</td>\n",
       "      <td>0.919602</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8       19.005445      0.294881         0.137581        0.010002   \n",
       "12      23.500025      0.129350         0.146319        0.004166   \n",
       "9       19.038350      0.098154         0.143256        0.014478   \n",
       "0       15.112634      0.768110         0.120299        0.014379   \n",
       "17      17.987884      0.055443         0.117686        0.002098   \n",
       "13      25.087349      0.098218         0.139582        0.013390   \n",
       "4       13.301558      0.157031         0.104106        0.003792   \n",
       "15      14.022394      0.057102         0.120369        0.016562   \n",
       "10      19.809258      0.091378         0.120953        0.004964   \n",
       "7       20.385061      0.076978         0.120643        0.002177   \n",
       "1        8.573562      0.168787         0.096001        0.002705   \n",
       "5       16.301050      0.136761         0.115175        0.005752   \n",
       "3       21.816092      0.286850         0.139232        0.011987   \n",
       "14      19.623811      0.229109         0.119622        0.004544   \n",
       "19      21.376662      0.299198         0.122625        0.002150   \n",
       "2       12.064271      0.025927         0.099042        0.001680   \n",
       "6       14.424132      0.170260         0.102681        0.001377   \n",
       "16      10.499972      0.030676         0.095751        0.000878   \n",
       "11      10.088888      0.065485         0.102079        0.007026   \n",
       "18       8.921600      0.173972         0.095163        0.000938   \n",
       "\n",
       "   param_subsample param_reg_lambda param_min_child_weight param_max_depth  \\\n",
       "8              1.0              1.2                      3               7   \n",
       "12             1.0              0.6                      7              10   \n",
       "9              0.6              0.6                      1               7   \n",
       "0              0.6              1.4                      7               6   \n",
       "17             0.8                1                      1               6   \n",
       "13             1.0              0.4                      1               9   \n",
       "4              1.0              0.6                     10               5   \n",
       "15             0.6                1                      5               5   \n",
       "10             0.6              0.4                      7               8   \n",
       "7              0.8                1                      3               7   \n",
       "1              1.0              0.8                      7               3   \n",
       "5              0.6              0.8                     10               6   \n",
       "3              1.0              1.2                     10               9   \n",
       "14             1.0              0.6                     10               8   \n",
       "19             1.0              1.4                     10              10   \n",
       "2              0.6              1.2                     10               4   \n",
       "6              1.0                1                      5               5   \n",
       "16             0.8              0.4                     10               3   \n",
       "11             0.6              1.2                      5               3   \n",
       "18             1.0              1.4                     10               3   \n",
       "\n",
       "   param_gamma param_eta param_colsample_bytree  \\\n",
       "8          1.5      0.03                    0.8   \n",
       "12         0.5      0.01                    0.6   \n",
       "9          0.5      0.01                    0.8   \n",
       "0            0      0.01                    0.6   \n",
       "17           1      0.01                    0.6   \n",
       "13           2      0.01                    0.6   \n",
       "4            1      0.01                    0.6   \n",
       "15           1     0.005                    0.6   \n",
       "10         1.5      0.01                    1.0   \n",
       "7          1.5     0.001                    0.8   \n",
       "1          0.5      0.03                    0.8   \n",
       "5          2.5      0.01                    1.0   \n",
       "3            1     0.005                    1.0   \n",
       "14         1.5     0.005                    1.0   \n",
       "19           2     0.005                    1.0   \n",
       "2          0.5     0.001                    0.8   \n",
       "6            2     0.005                    1.0   \n",
       "16         2.5     0.001                    0.6   \n",
       "11           2     0.001                    0.8   \n",
       "18           1     0.005                    1.0   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "8   {'subsample': 1.0, 'reg_lambda': 1.2, 'min_chi...           0.950181   \n",
       "12  {'subsample': 1.0, 'reg_lambda': 0.6, 'min_chi...           0.947933   \n",
       "9   {'subsample': 0.6, 'reg_lambda': 0.6, 'min_chi...           0.947627   \n",
       "0   {'subsample': 0.6, 'reg_lambda': 1.4, 'min_chi...           0.947403   \n",
       "17  {'subsample': 0.8, 'reg_lambda': 1, 'min_child...           0.948115   \n",
       "13  {'subsample': 1.0, 'reg_lambda': 0.4, 'min_chi...           0.947056   \n",
       "4   {'subsample': 1.0, 'reg_lambda': 0.6, 'min_chi...           0.945127   \n",
       "15  {'subsample': 0.6, 'reg_lambda': 1, 'min_child...           0.944343   \n",
       "10  {'subsample': 0.6, 'reg_lambda': 0.4, 'min_chi...           0.943793   \n",
       "7   {'subsample': 0.8, 'reg_lambda': 1, 'min_child...           0.943390   \n",
       "1   {'subsample': 1.0, 'reg_lambda': 0.8, 'min_chi...           0.942501   \n",
       "5   {'subsample': 0.6, 'reg_lambda': 0.8, 'min_chi...           0.940976   \n",
       "3   {'subsample': 1.0, 'reg_lambda': 1.2, 'min_chi...           0.935796   \n",
       "14  {'subsample': 1.0, 'reg_lambda': 0.6, 'min_chi...           0.935788   \n",
       "19  {'subsample': 1.0, 'reg_lambda': 1.4, 'min_chi...           0.936308   \n",
       "2   {'subsample': 0.6, 'reg_lambda': 1.2, 'min_chi...           0.935148   \n",
       "6   {'subsample': 1.0, 'reg_lambda': 1, 'min_child...           0.931463   \n",
       "16  {'subsample': 0.8, 'reg_lambda': 0.4, 'min_chi...           0.930889   \n",
       "11  {'subsample': 0.6, 'reg_lambda': 1.2, 'min_chi...           0.926852   \n",
       "18  {'subsample': 1.0, 'reg_lambda': 1.4, 'min_chi...           0.919542   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "8            0.950860           0.950343         0.950461        0.000290   \n",
       "12           0.948925           0.948339         0.948399        0.000407   \n",
       "9            0.948614           0.948234         0.948158        0.000406   \n",
       "0            0.948768           0.948019         0.948064        0.000558   \n",
       "17           0.948486           0.947454         0.948018        0.000427   \n",
       "13           0.947549           0.946818         0.947141        0.000305   \n",
       "4            0.946633           0.946009         0.945923        0.000618   \n",
       "15           0.945689           0.944817         0.944949        0.000557   \n",
       "10           0.944025           0.944904         0.944241        0.000479   \n",
       "7            0.945132           0.943979         0.944167        0.000723   \n",
       "1            0.943380           0.942942         0.942941        0.000359   \n",
       "5            0.940614           0.942232         0.941274        0.000693   \n",
       "3            0.935603           0.937142         0.936180        0.000685   \n",
       "14           0.935769           0.936873         0.936144        0.000516   \n",
       "19           0.935201           0.936711         0.936073        0.000638   \n",
       "2            0.936426           0.936339         0.935971        0.000583   \n",
       "6            0.931666           0.933989         0.932373        0.001146   \n",
       "16           0.931758           0.930827         0.931158        0.000425   \n",
       "11           0.926982           0.927126         0.926987        0.000112   \n",
       "18           0.917448           0.921817         0.919602        0.001784   \n",
       "\n",
       "    rank_test_score  \n",
       "8                 1  \n",
       "12                2  \n",
       "9                 3  \n",
       "0                 4  \n",
       "17                5  \n",
       "13                6  \n",
       "4                 7  \n",
       "15                8  \n",
       "10                9  \n",
       "7                10  \n",
       "1                11  \n",
       "5                12  \n",
       "3                13  \n",
       "14               14  \n",
       "19               15  \n",
       "2                16  \n",
       "6                17  \n",
       "16               18  \n",
       "11               19  \n",
       "18               20  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imbalanced classification report on optimized xgboost using roc auc as a metrics system\n",
    "print(imblearn.metrics.classification_report_imbalanced(train_data[\"labels\"],\n",
    "                                                  rs_clf.predict(train_data[data_cols]),\n",
    "                                                  target_names=le.classes_))\n",
    "\n",
    "print(imblearn.metrics.classification_report_imbalanced(test_data[\"labels\"],\n",
    "                                                  rs_clf.predict(test_data[data_cols]),\n",
    "                                                  target_names=le.classes_))\n",
    "\n",
    "pd.DataFrame(rs_clf.cv_results_).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "frank-fusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\core.py:419: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning)\n",
      "C:\\Users\\yahoy\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making xgboost classifier using the best parameter\n",
    "rs_roc_auc_ovr = pd.DataFrame(rs_clf.cv_results_)\n",
    "best_params = rs_clf.best_params_\n",
    "best_params.update(param)\n",
    "\n",
    "bst = xgb.XGBClassifier(best_params)\n",
    "bst.fit(train_data[data_cols], train_data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "permanent-trademark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.88      0.77      0.99      0.82      0.87      0.75      3114\n",
      "            Emo       0.95      0.97      1.00      0.96      0.98      0.96      1140\n",
      "         Hiphop       0.83      0.77      0.99      0.79      0.87      0.74      2064\n",
      "            Pop       0.94      0.87      1.00      0.90      0.93      0.86       300\n",
      "            Rap       0.87      0.66      1.00      0.75      0.81      0.63      1276\n",
      "            RnB       0.81      0.81      0.99      0.81      0.89      0.79      1398\n",
      "     Trap Metal       0.82      0.70      0.99      0.75      0.83      0.67      1336\n",
      "Underground Rap       0.68      0.85      0.94      0.76      0.89      0.79      3948\n",
      "            dnb       1.00      1.00      1.00      1.00      1.00      1.00      2026\n",
      "      hardstyle       0.99      1.00      1.00      1.00      1.00      1.00      1977\n",
      "      psytrance       1.00      1.00      1.00      1.00      1.00      1.00      1932\n",
      "      techhouse       0.99      0.99      1.00      0.99      1.00      0.99      2055\n",
      "         techno       0.99      0.99      1.00      0.99      1.00      0.99      1999\n",
      "         trance       0.99      1.00      1.00      0.99      1.00      1.00      1969\n",
      "           trap       0.99      1.00      1.00      0.99      1.00      1.00      2017\n",
      "\n",
      "    avg / total       0.90      0.89      0.99      0.90      0.94      0.88     28551\n",
      "\n",
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.53      0.48      0.95      0.50      0.67      0.43      1133\n",
      "            Emo       0.72      0.70      0.99      0.71      0.83      0.67       415\n",
      "         Hiphop       0.44      0.41      0.96      0.43      0.63      0.37       747\n",
      "            Pop       0.22      0.09      1.00      0.12      0.29      0.08       117\n",
      "            Rap       0.51      0.34      0.99      0.41      0.58      0.31       447\n",
      "            RnB       0.43      0.38      0.97      0.40      0.61      0.35       541\n",
      "     Trap Metal       0.38      0.30      0.98      0.34      0.54      0.27       468\n",
      "Underground Rap       0.42      0.56      0.87      0.48      0.70      0.48      1495\n",
      "            dnb       0.97      0.99      1.00      0.98      0.99      0.98       716\n",
      "      hardstyle       0.88      0.95      0.99      0.91      0.97      0.93       731\n",
      "      psytrance       0.93      0.93      0.99      0.93      0.96      0.92       789\n",
      "      techhouse       0.89      0.89      0.99      0.89      0.94      0.87       697\n",
      "         techno       0.87      0.87      0.99      0.87      0.93      0.85       752\n",
      "         trance       0.85      0.87      0.99      0.86      0.93      0.85       783\n",
      "           trap       0.89      0.84      0.99      0.87      0.91      0.82       744\n",
      "\n",
      "    avg / total       0.68      0.68      0.97      0.68      0.80      0.65     10575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report on xgboost with best param\n",
    "print(imblearn.metrics.classification_report_imbalanced(train_data[\"labels\"],bst.predict(train_data[data_cols]),\n",
    "                                                  target_names = le.classes_))\n",
    "print(imblearn.metrics.classification_report_imbalanced(test_data[\"labels\"],bst.predict(test_data[data_cols]),\n",
    "                                                  target_names = le.classes_))\n",
    "best_params.update(param);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "changing-luxury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tempo</th>\n",
       "      <td>0.235620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instrumentalness</th>\n",
       "      <td>0.153189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration_ms</th>\n",
       "      <td>0.147276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danceability</th>\n",
       "      <td>0.098365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loudness</th>\n",
       "      <td>0.077594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energy</th>\n",
       "      <td>0.058657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speechiness</th>\n",
       "      <td>0.051928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.049582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acousticness</th>\n",
       "      <td>0.043925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode</th>\n",
       "      <td>0.029884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liveness</th>\n",
       "      <td>0.023133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_signature</th>\n",
       "      <td>0.018503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <td>0.012345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feat_importance\n",
       "tempo                    0.235620\n",
       "instrumentalness         0.153189\n",
       "duration_ms              0.147276\n",
       "danceability             0.098365\n",
       "loudness                 0.077594\n",
       "energy                   0.058657\n",
       "speechiness              0.051928\n",
       "valence                  0.049582\n",
       "acousticness             0.043925\n",
       "mode                     0.029884\n",
       "liveness                 0.023133\n",
       "time_signature           0.018503\n",
       "key                      0.012345"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the feature importance extracted from best xgboost model\n",
    "pd.DataFrame([{\"feat_importance\":importance} for importance, dcol in zip(rs_clf.best_estimator_.feature_importances_, data_cols)], index=data_cols).sort_values(\"feat_importance\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "industrial-century",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "knn_clf.fit(train_data[data_cols], train_data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "original-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.32      0.42      0.89      0.36      0.61      0.36      3114\n",
      "            Emo       0.28      0.23      0.98      0.25      0.47      0.21      1140\n",
      "         Hiphop       0.32      0.29      0.95      0.30      0.52      0.26      2064\n",
      "            Pop       0.30      0.04      1.00      0.07      0.20      0.04       300\n",
      "            Rap       0.32      0.14      0.99      0.20      0.38      0.13      1276\n",
      "            RnB       0.31      0.16      0.98      0.21      0.39      0.14      1398\n",
      "     Trap Metal       0.39      0.25      0.98      0.30      0.49      0.22      1336\n",
      "Underground Rap       0.41      0.48      0.89      0.45      0.66      0.41      3948\n",
      "            dnb       0.46      0.63      0.94      0.53      0.77      0.58      2026\n",
      "      hardstyle       0.37      0.38      0.95      0.38      0.60      0.34      1977\n",
      "      psytrance       0.55      0.76      0.95      0.64      0.85      0.72      1932\n",
      "      techhouse       0.44      0.45      0.96      0.45      0.66      0.41      2055\n",
      "         techno       0.50      0.45      0.97      0.47      0.66      0.41      1999\n",
      "         trance       0.37      0.21      0.97      0.27      0.45      0.19      1969\n",
      "           trap       0.45      0.51      0.95      0.48      0.69      0.46      2017\n",
      "\n",
      "    avg / total       0.40      0.41      0.94      0.39      0.60      0.36     28551\n",
      "\n",
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.18      0.25      0.87      0.21      0.46      0.20      1133\n",
      "            Emo       0.10      0.07      0.97      0.08      0.26      0.06       415\n",
      "         Hiphop       0.14      0.14      0.93      0.14      0.36      0.12       747\n",
      "            Pop       0.05      0.01      1.00      0.01      0.09      0.01       117\n",
      "            Rap       0.11      0.05      0.98      0.07      0.22      0.04       447\n",
      "            RnB       0.12      0.06      0.98      0.08      0.24      0.05       541\n",
      "     Trap Metal       0.22      0.14      0.98      0.17      0.37      0.13       468\n",
      "Underground Rap       0.28      0.32      0.87      0.30      0.52      0.26      1495\n",
      "            dnb       0.35      0.50      0.93      0.41      0.68      0.45       716\n",
      "      hardstyle       0.24      0.25      0.94      0.24      0.48      0.22       731\n",
      "      psytrance       0.54      0.72      0.95      0.62      0.83      0.67       789\n",
      "      techhouse       0.31      0.32      0.95      0.32      0.55      0.28       697\n",
      "         techno       0.40      0.38      0.96      0.39      0.60      0.34       752\n",
      "         trance       0.22      0.12      0.97      0.16      0.34      0.11       783\n",
      "           trap       0.38      0.43      0.95      0.40      0.64      0.38       744\n",
      "\n",
      "    avg / total       0.27      0.29      0.93      0.27      0.49      0.25     10575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(imblearn.metrics.classification_report_imbalanced(train_data[\"labels\"],knn_clf.predict(train_data[data_cols]),\n",
    "                                                  target_names = le.classes_))\n",
    "print(imblearn.metrics.classification_report_imbalanced(test_data[\"labels\"],knn_clf.predict(test_data[data_cols]),\n",
    "                                                  target_names = le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "particular-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.55      0.54      0.95      0.55      0.71      0.49      3114\n",
      "            Emo       0.71      0.69      0.99      0.70      0.82      0.66      1140\n",
      "         Hiphop       0.52      0.42      0.97      0.47      0.64      0.38      2064\n",
      "            Pop       0.32      0.13      1.00      0.18      0.36      0.12       300\n",
      "            Rap       0.64      0.35      0.99      0.45      0.59      0.32      1276\n",
      "            RnB       0.43      0.44      0.97      0.44      0.65      0.41      1398\n",
      "     Trap Metal       0.58      0.27      0.99      0.37      0.52      0.25      1336\n",
      "Underground Rap       0.48      0.63      0.89      0.54      0.75      0.55      3948\n",
      "            dnb       0.95      0.98      1.00      0.97      0.99      0.98      2026\n",
      "      hardstyle       0.84      0.91      0.99      0.87      0.95      0.89      1977\n",
      "      psytrance       0.92      0.91      0.99      0.91      0.95      0.89      1932\n",
      "      techhouse       0.84      0.89      0.99      0.86      0.94      0.87      2055\n",
      "         techno       0.85      0.86      0.99      0.86      0.92      0.84      1999\n",
      "         trance       0.77      0.88      0.98      0.82      0.93      0.85      1969\n",
      "           trap       0.80      0.85      0.98      0.82      0.91      0.82      2017\n",
      "\n",
      "    avg / total       0.69      0.69      0.97      0.68      0.81      0.66     28551\n",
      "\n",
      "                       pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "      Dark Trap       0.49      0.49      0.94      0.49      0.68      0.44      1133\n",
      "            Emo       0.63      0.61      0.99      0.62      0.78      0.58       415\n",
      "         Hiphop       0.45      0.38      0.97      0.41      0.60      0.34       747\n",
      "            Pop       0.31      0.12      1.00      0.17      0.35      0.11       117\n",
      "            Rap       0.58      0.32      0.99      0.42      0.57      0.30       447\n",
      "            RnB       0.40      0.37      0.97      0.38      0.60      0.34       541\n",
      "     Trap Metal       0.56      0.26      0.99      0.36      0.51      0.24       468\n",
      "Underground Rap       0.47      0.61      0.89      0.53      0.74      0.53      1495\n",
      "            dnb       0.94      0.96      1.00      0.95      0.98      0.95       716\n",
      "      hardstyle       0.81      0.89      0.98      0.85      0.94      0.87       731\n",
      "      psytrance       0.92      0.91      0.99      0.91      0.95      0.90       789\n",
      "      techhouse       0.82      0.87      0.99      0.84      0.92      0.84       697\n",
      "         techno       0.86      0.84      0.99      0.85      0.91      0.82       752\n",
      "         trance       0.75      0.86      0.98      0.80      0.92      0.84       783\n",
      "           trap       0.77      0.81      0.98      0.79      0.89      0.78       744\n",
      "\n",
      "    avg / total       0.66      0.67      0.97      0.66      0.79      0.63     10575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training multilayer classifier\n",
    "# scaling since neural networks need variabels to be scaled\n",
    "ss = preprocessing.StandardScaler()\n",
    "ss_train_data = ss.fit_transform(train_data[data_cols])\n",
    "ss_test_data = ss.transform(test_data[data_cols])\n",
    "mlp = neural_network.MLPClassifier(hidden_layer_sizes = (200,50), learning_rate = \"adaptive\",\n",
    "                                  max_iter=3000,early_stopping = True)\n",
    "mlp.fit(ss_train_data, train_data[\"labels\"])\n",
    "print(imblearn.metrics.classification_report_imbalanced(train_data[\"labels\"],mlp.predict(ss_train_data),\n",
    "                                                  target_names = le.classes_))\n",
    "print(imblearn.metrics.classification_report_imbalanced(test_data[\"labels\"],mlp.predict(ss_test_data),\n",
    "                                                  target_names = le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "found-prompt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8245094316157416"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss_ # the loss funciton defaults to adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "religious-frame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Dark Trap       0.49      0.49      0.49      1133\n",
      "            Emo       0.63      0.61      0.62       415\n",
      "         Hiphop       0.45      0.38      0.41       747\n",
      "            Pop       0.31      0.12      0.17       117\n",
      "            Rap       0.58      0.32      0.42       447\n",
      "            RnB       0.40      0.37      0.38       541\n",
      "     Trap Metal       0.56      0.26      0.36       468\n",
      "Underground Rap       0.47      0.61      0.53      1495\n",
      "            dnb       0.94      0.96      0.95       716\n",
      "      hardstyle       0.81      0.89      0.85       731\n",
      "      psytrance       0.92      0.91      0.91       789\n",
      "      techhouse       0.82      0.87      0.84       697\n",
      "         techno       0.86      0.84      0.85       752\n",
      "         trance       0.75      0.86      0.80       783\n",
      "           trap       0.77      0.81      0.79       744\n",
      "\n",
      "       accuracy                           0.67     10575\n",
      "      macro avg       0.65      0.62      0.63     10575\n",
      "   weighted avg       0.66      0.67      0.66     10575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for mlp for test set\n",
    "print(metrics.classification_report(test_data[\"labels\"],mlp.predict(ss_test_data),\n",
    "                                                  target_names = le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-wallace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
